---
layout: post
title: Stating the obvious consequences
---

Sometimes authors don't state the far-reaching consequences of their work.
This could be humility or could be that they perceive it as obvious.

Such a norm seems to be quite prevalent in maths. I think there's also a lot
of signalling going on not stating the obvious: I guess that's one way to
project smartness.

I recall the following conversation from my undergraduate lecture:

> Me: I really like this result in algebraic number theory showing that
> triplets of real numbers can not be turned into a [field][field].
>
> Friend: But why would you even want to turn triplets into a field!?
>
> Me: You remember you told me that complex analysis is your favourite topic at
> the moment? Come to think of it, most of it is only possible due to the fact
> that we turned pairs of real numbers into a field. Perhaps if we could the
> same with triples we'd develop another fascinating subject. But we won't as
> it's not possible.
>
> Friend: I never realised such consequence, I guess such investigation makes
> sense now.

However the above motivation behind investigating whether triplets could be
turned into a field wasn't stated in the course. It wasn't that the lecturer
didn't have the time. It's just the way things are done in many courses at the
moment: we are presented with results often without their significance
explicitly stated.

----------

So I am going to state significance behind things that are of interest to
me explicitly right here, right now.

# [Cox's theorem][cox]

Cox's theorem says: there's one, objective, reality: out there to be
discovered. Your job is to approximate such reality with the information that
you have. Since you can only have limited amount of information, you need a
theory to deal with uncertainty. Turns out probability theory is a unique
way to deal with uncertainty.

Roughly speaking, if you want to adhere to some mild assumptions of reasoning,
then you should assign a probability to every statement that you believe, *as a
consequence*. And then update such number according to the laws of probability
theory when you encounter new information. The higher probability you assign,
the more confident you are. Of course, in practice you should only strive to
approximate this framework, as the mother nature didn't equip our human
brains with such calculators.

Why is this highly significant? Philosophers argued for centuries about
different epistemologies, developed and spread retarded memes as a result.

Such as: you can never be sure of anything or every belief is valid. Such
ideas blatantly violate the conclusion of the Cox's theorem: the certainty
you assign to believes does not have only 3 values: (true, false, I don't know),
it has an entire uncoutable range of them.  I wrote more about it in
[Plausibility = Just Double | DoNotKnow][idontknow]. And it's all right there,
in the boring equations. Work of a dozen of philosophers undermined by one
physicist, R. T. Cox.

Another thing about Cox's theorem is that it is single-valued. Turns out how
human feel about a statement: whether it's offensive or noble doesn't affect
it's veracity. Again, this is all just a consequence of the proof. So taking
an offence doesn't disprove a statement and people identifying with social
justice movement ought to start taking this into account.

# Work by Daniel Kahneman

More and more people I speak to read "Thinking fast and slow". And then yawned
and went back to living their lives the way they lived before. I hear the
following sentiments:

> You are telling me that people, en-masse, are irrational? Who would've
> thought!

Daniel's mighty thesis that people are, in fact, irrational is hard to dispute.
But there's more to it than just that. Together with Tversky he identified some
areas where people are irrational and guided us how to avoid such common
pitfalls. And, perhaps, some readers take notice and adjust.

However, what really *should* be happening is the following:

> Oh my god, everything I've ever believed in and ever argued for could well be
> deeply mistaken. Time and time again Daniel's book highlighted that our
> intuitions are often wrong. This wasted millions of man-hours spent on
> achieving stuff ineffectively people didn't *really* want to achieve had they
> thought about it more logically. I *need* some new, solid knowledge on how to
> be sure that I avoid similar traps.

-----------

Turns out Cox's theorem and Daniel's work go hand in hand together. For once
you believe that you need a systematic way to arrive at epistemic truths
without System 1 interfering, Cox's theorem fits the bill.

People who were thinking about the theory behind Artificial Intelligence also
ran into this problem. How to teach a computer to reason in uncertainty
and how to make it value what we, humans, value?

The technical details of all of this haven't yet been worked out, but a
first-order approximation framework began to emerge. And people in charge of
such theoretical AI designs needed to unite some work in epistemology,
decision-making, game-theory, ethics and more for their answers.

However, as a nice side effect such people stumbled upon ways how to make, us,
humans more strategic: we should just strive to approximate such idealised
framework, especially in the areas where the effect of System 1 is particularly
strong.

This is how [lesswrong][LW] was born in 2009 which provided some of the answers
to:

> OMG my brain is a massive liar what do I do now?

Your welcome.

[field]: https://en.wikipedia.org/wiki/Field_(mathematics)
[cox]: https://en.wikipedia.org/wiki/Cox%27s_theorem
[idontknow]: https://afiodorov.github.io/rationality/2015/11/18/just-plausibility/
[LW]: https://wiki.lesswrong.com/wiki/FAQ
